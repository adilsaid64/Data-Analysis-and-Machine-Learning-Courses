{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adilsaid64/Data-Analysis-and-Machine-Learning-Courses/blob/main/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd #used to precess data sets and import them"
      ],
      "metadata": {
        "id": "CvaX6vDFZela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In any data set where you are going to train a machine learning model, you have features, and depedenat variables.\n",
        "\n",
        "features are -independant variables.dependant variable - in this case, if the customer purchacesd. usally the last column in data set. "
      ],
      "metadata": {
        "id": "b_kpY03JhuQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"Data.csv\") #data frame\n",
        "\n",
        "#create matrix of features and dependent variable vector.\n",
        "\n",
        "#all the rows, and all the columns except the last one. rows --, column |\n",
        "X = dataset.iloc[:, :-1].values\n",
        "\n",
        "#takes the last column\n",
        "y = dataset.iloc[:,-1].values\n"
      ],
      "metadata": {
        "id": "3GZdyVsJa81w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Jo4lGBo8Wt",
        "outputId": "e66fbc68-61a8-4938-f272-48c9a6e99da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv3D-9IGo_SR",
        "outputId": "ddc593fc-8aa5-40ba-fef3-2d1be9d1454b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Removing data, sometimes in large data sets you can just delete the entire entry that contains the missing data. Removing a few entires from a large data wont affect it much. this is one moethod.\n",
        "\n",
        "*   Replacing missing values by average of all varialbes in column. Classic way of handling missing data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kfkqmcN6PWNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer #class\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") #this is an object of the class\n",
        "imputer.fit(X[:, 1:3])\n",
        "imputer.transform(X[:, 1:3])\n",
        "\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "yDTr6BqnP1sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYoV6UBASO_V",
        "outputId": "aa54d9e0-e1aa-4240-bbc0-b011002d855b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data (Dummy Variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable\n",
        "\n",
        "*   Encoding them as 0,1,2 can have potential and missinterpretation by the model. It might start odering the indepentant catorical varialbes.\n",
        "*   One hut encoding. Turning into 3 columns, since we have 3 differnt catogories. Binary Vectors for each of the county\n",
        "\n",
        "*   This way we use vectors for example <France, Spain, German>, So if we want France we do <1,0,0> etc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "OWyEmMOx4Qrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXHroxiW4j-7",
        "outputId": "597ab608-c7cc-4058-c77d-c7108130c97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use Binary for the Yes and no.so 1,0."
      ],
      "metadata": {
        "id": "yeEQ-fRo46i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "fd6mnUjC4_wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCCKwPN25V-O",
        "outputId": "2d559a19-69d3-4df7-9d6c-ac5042ed2b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will get 4 sets.\n",
        "2 from the training data and 2 from the testing data.\n",
        "matrix of features and dependant variables sets from both datasets. This is the format expected by the model.\n"
      ],
      "metadata": {
        "id": "7wqMFJW-C0-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state  =1)#random state sets the seed to not get different results."
      ],
      "metadata": {
        "id": "JrEjY-DdCt7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKuU65STEYl_",
        "outputId": "053a6457-f563-4f81-8c76-bb819d1eb791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU-hOP9EEYs6",
        "outputId": "1ec5c321-4e68-40c3-e498-8c38fd20e741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGT8VrCLEYzK",
        "outputId": "dd98b561-a51f-4ef3-8386-fffab299368e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwC5dbV-EY48",
        "outputId": "f0aa80f6-4be2-486c-bce6-b6254978cbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling all features to make sure they take values in the same scale to prevent one feature from dominating the other.\n",
        "This occurs after. Test set is supposed to be considered a brand new set, to evaluate the model. So this is the reason we dont work on the test set. We treat it as new data. Doing Feature Scaling first, results in information leakage."
      ],
      "metadata": {
        "id": "mR2RRD94CJwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of feature scaling.\n",
        "\n",
        " \n",
        "\n",
        "*   Normalisation - when you have a normal distribution in most of features.(Specific sitiation)\n",
        " \n",
        "*   Standardisation - works well all the time. Values are between -3 and +3\n",
        "\n",
        "\n",
        "*   Dont apply to Catogorical Dummy Varialbes.\n",
        "\n"
      ],
      "metadata": {
        "id": "3WoOe5beZ0BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "#from column 3 and onwards to avoid scaling catogorical data\n",
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
        "X_test[:,3:] = sc.transform(X_test[:,3:])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qT42l9xBCr-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhumjj0Ddxyo",
        "outputId": "ab5b1b37-d735-44c2-a4b1-ecc65899709b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xTCljL5d0Fx",
        "outputId": "74db606c-e777-42f4-9f83-9e0f6a650812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    }
  ]
}
